{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Exercise\n",
    "\n",
    "# California Housing Data\n",
    "\n",
    "This data set contains information about all the block groups in California from the 1990 Census. In this sample a block group on average includes 1425.5 individuals living in a geographically compact area.\n",
    "\n",
    "The task is to aproximate the median house value of each block from the values of the rest of the variables.\n",
    "\n",
    "It has been obtained from the LIACC repository. The original page where the data set can be found is: http://www.liaad.up.pt/~ltorgo/Regression/DataSets.html.\n",
    "\n",
    "The Features:\n",
    "\n",
    "housingMedianAge: continuous.\n",
    "\n",
    "totalRooms: continuous.\n",
    "\n",
    "totalBedrooms: continuous.\n",
    "\n",
    "population: continuous.\n",
    "\n",
    "households: continuous.\n",
    "\n",
    "medianIncome: continuous.\n",
    "\n",
    "medianHouseValue: continuous.\n",
    "\n",
    "\n",
    "# The Data\n",
    "Import the cal_housing_clean.csv file with pandas. Separate it into a training (70%) and testing set(30%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as plt\n",
    "from sklearn import model_selection\n",
    "\n",
    "data  ==  pd.read_csvread_cs ('California Housing data.csv')\n",
    "data = data.dropna(axis=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(labels='Unnamed: 0', inplace=True, axis=1)\n",
    "data.head().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_data = data['medianHouseValue']\n",
    "data.drop(labels=['medianHouseValue'], inplace=True, axis =1)\n",
    "x_data = data\n",
    "\n",
    "\n",
    "X_trainX_train, X_test, y_train, y_test = model_selection.train_test_split(x_data, y_data, test_size=0.3, random_state=1)\n",
    "\n",
    "\n",
    "y_trainy_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scale the Feature Data\n",
    "Use sklearn preprocessing to create a MinMaxScaler for the feature data. Fit this scaler only to the training data. Then use it to transform X_test and X_train. Then use the scaled X_test and X_train along with pd.Dataframe to re-create two dataframes of scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Feature Columns\n",
    "Create the necessary tf.feature_column objects for the estimator. They should all be trated as continuous numeric_columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(data=X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test = pd.DataFrame(data=X_test_scaled, columns=X_test.columns, index=X_test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "house_age = tf.feature_column.numeric_column('HouseAge')\n",
    "av_rooms = tf.feature_column.numeric_column('AveRooms')\n",
    "av_bedroom = tf.feature_column.numeric_column('AveBedrms')\n",
    "population = tf.feature_column.numeric_column('Population')\n",
    "av_occup = tf.feature_column.numeric_column('AveOccup')\n",
    "med_income = tf.feature_column.numeric_column('MedInc')\n",
    "longitude = tf.feature_column.numeric_column('Longitude')\n",
    "latitude = tf.feature_column.numeric_column('Latitude')\n",
    "\n",
    "feat_columns = [house_age, av_rooms, av_bedroom, population, av_occup, med_income, longitude, latitude]\n",
    "feat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_func = tf.estimator.inputs.pandas_input_fn(x=X_train, y=y_train, batch_size=10,num_epochs=1000,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model  ==  tf.estimator .DNNRegressor(hidden_units=[10, 12, 14, 12, 10], feature_columns=feat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model for ~1,000 steps. (Later come back to this and train it for more and check for improvement) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(input_fn = input_func, steps = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a prediction input function and then use the .predict method off your estimator model to create a list or predictions on your test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_input_func = tf.estimator.inputs.pandas_input_fn(x=X_test, batch_size=20,num_epochs=1,shuffle=False)\n",
    "\n",
    "pred_model = model.predict(pred_input_func)\n",
    "preds = list(pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_preds = []\n",
    "for pred in preds:\n",
    "    final_y_preds.append(pred['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_y_preds[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the RMSE. You should be able to get around 100,000 RMSE (remember that this is in the same units as the label.) Do this manually or use sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "import math\n",
    "\n",
    "rmse = math.sqrt(metrics.mean_squared_error(y_test, final_y_preds))\n",
    "rmse"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
